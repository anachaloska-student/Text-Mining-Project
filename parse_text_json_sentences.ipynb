{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langdetect import detect\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Files\n",
    "TEXT_FILE = 'review_copy.txt'\n",
    "OUTPUT_FILE = 'review_sentences.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parses the data\n",
    "reviews = []\n",
    "with open(TEXT_FILE, 'rb') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        reviews.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for review in reviews:\n",
    "    for key in ('title','author', 'date_stayed', 'offering_id', 'num_helpful_votes', 'date', 'id', 'via_mobile'):\n",
    "        if key in review:\n",
    "            del review[key]\n",
    "    review['text']=review['text'].lower()\n",
    "    if not detect(review['text'])=='en':\n",
    "        reviews.remove(review)\n",
    "    if len(review['ratings'])!=7:\n",
    "        reviews.remove(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collects all the textual reviews in a list of lists with sentences\n",
    "all_reviews = []\n",
    "for review in reviews:\n",
    "    all_reviews.append(re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', review['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_tokens = []\n",
    "\n",
    "for review in all_reviews:\n",
    "    word_tokens_sentence = []\n",
    "    for sentence in review:\n",
    "        word_tokens_sentence.append(word_tokenize(sentence))\n",
    "    word_tokens.append(word_tokens_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Based on some papers we looked at, we manually removed some of the english stopwords which could affect our emotion analysis \n",
    "stop_words = set(stopwords.words('english'))-set(('no', 'not', 'didn', 'doesn', 'don', 'down', 'hasn', 'haven'))\n",
    "\n",
    "filtered_reviews = []\n",
    "for word_token in word_tokens:\n",
    "    filtered_reviews_sentence = []\n",
    "    for word_token_sentence in word_token:\n",
    "        filtered_reviews_sentence.append([w for w in word_token_sentence if not w in stop_words])\n",
    "    filtered_reviews.append(filtered_reviews_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removes the non-alphabetic characters \n",
    "isalpha_filtered_reviews = []\n",
    "for review in filtered_reviews:\n",
    "    isalpha_filtered_sentences = []\n",
    "    for sentence in review:\n",
    "        isalpha_sentence = []\n",
    "        for word in sentence:\n",
    "            #isalpha_review.append(regex.sub('', word))\n",
    "            if word.isalpha() == True:\n",
    "                isalpha_sentence.append(word)\n",
    "        isalpha_filtered_sentences.append(isalpha_sentence)\n",
    "    isalpha_filtered_reviews.append(isalpha_filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_reviews = isalpha_filtered_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# POS-Tagging and Lemmatization\n",
    "un2wn_mapping = {\"VERB\" : wn.VERB, \"NOUN\" : wn.NOUN, \"ADJ\" : wn.ADJ, \"ADV\" : wn.ADV}\n",
    "\n",
    "docs = []\n",
    "\n",
    "for review in all_reviews:\n",
    "    lemmatized_reviews = []\n",
    "    for sentence in review:\n",
    "        lemmatized_sentence = []\n",
    "        for w, p in nltk.pos_tag(sentence, tagset=\"universal\"):\n",
    "            if p in un2wn_mapping.keys():\n",
    "                lemma = nltk.WordNetLemmatizer().lemmatize(w, pos = un2wn_mapping[p])\n",
    "            else:\n",
    "                lemma = nltk.WordNetLemmatizer().lemmatize(w)\n",
    "\n",
    "            lemmatized_sentence.append(lemma.lower())  # case insensitive\n",
    "        lemmatized_reviews.append(lemmatized_sentence)\n",
    "    docs.append(lemmatized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
