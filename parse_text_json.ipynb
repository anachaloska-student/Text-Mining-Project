{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3new\\lib\\site-packages\\gensim-3.1.0-py3.6-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Slow version of gensim.models.doc2vec is being used\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langdetect import detect\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Files\n",
    "TEXT_FILE = 'review_copy.txt'\n",
    "OUTPUT_FILE = 'review_normal.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parses the data\n",
    "reviews = []\n",
    "with open(TEXT_FILE, 'rb') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        reviews.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in reviews:\n",
    "    for key in ('title','author', 'date_stayed', 'offering_id', 'num_helpful_votes', 'date', 'id', 'via_mobile'):\n",
    "        if key in review:\n",
    "            del review[key]\n",
    "    review['text']=review['text'].lower()\n",
    "    if not detect(review['text'])=='en':\n",
    "        reviews.remove(review)\n",
    "    if len(review['ratings'])!=7:\n",
    "        reviews.remove(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collects all the textual reviews in a list\n",
    "all_reviews=[]\n",
    "for review in reviews:\n",
    "    all_reviews.append(review['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on some papers I looked at, I manually removed some of the english stopwords which could affect our emotion analysis \n",
    "stop_words = set(stopwords.words('english'))-set(('no', 'not', 'didn', 'doesn', 'don', 'down', 'hasn', 'haven'))\n",
    "word_tokens = []\n",
    "for review in all_reviews:\n",
    "    word_tokens.append(word_tokenize(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_sentences=[]\n",
    "for word_token in word_tokens:\n",
    "    filtered_sentences.append([w for w in word_token if not w in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removes the non-alphabetic characters \n",
    "isalpha_filtered_sentences=[]\n",
    "for sentence in filtered_sentences:\n",
    "    isalpha_sentence=[]\n",
    "    for word in sentence:\n",
    "        #isalpha_sentence.append(regex.sub('', word))\n",
    "        if word.isalpha()==True:\n",
    "            isalpha_sentence.append(word)\n",
    "    isalpha_filtered_sentences.append(isalpha_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_reviews=isalpha_filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# POS-Tagging and Lemmatization\n",
    "un2wn_mapping = {\"VERB\" : wn.VERB, \"NOUN\" : wn.NOUN, \"ADJ\" : wn.ADJ, \"ADV\" : wn.ADV}\n",
    "\n",
    "docs = []\n",
    "\n",
    "for review in all_reviews:\n",
    "    lemmatized_reviews = []\n",
    "    for w, p in nltk.pos_tag(review, tagset=\"universal\"):\n",
    "        if p in un2wn_mapping.keys():\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w, pos = un2wn_mapping[p])\n",
    "        else:\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w)\n",
    "            \n",
    "        lemmatized_reviews.append(lemma.lower())  # case insensitive\n",
    "        \n",
    "    docs.append(lemmatized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
